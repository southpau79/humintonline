{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5skCvmbYujon"
   },
   "source": [
    "Our goal is to go from what we will describe as a chunk of text (not to be confused with text chunking), a lengthy, unprocessed single string, and end up with a list (or several lists) of cleaned tokens that would be useful for further text mining and/or natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qPqxsUmuvJv"
   },
   "source": [
    "- NLTK - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, and beyond\n",
    "\n",
    "- BeautifulSoup - BeautifulSoup is a useful library for extracting data from HTML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qseduth-xXTY"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import re, string, unicodedata\n",
    "import nltk                                   # Natural language processing tool-kit\n",
    "\n",
    "!pip install contractions\n",
    "import contractions\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup                 # Beautiful soup is a parsing library that can use different parsers.\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet    # Stopwords, and wordnet corpus\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z56oRBGgu2ZW"
   },
   "source": [
    "We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBoMiEguukgC"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"<h1>This is the title</h1>\n",
    "            <b>This is bold text</b>\n",
    "            <i>This is italicized Text</i>\n",
    "            <img src=\"another html tag\"/>\n",
    "            <a href=\"Apart from the others\"> This is also here!</a>\n",
    "            “Love all, trust a few, do wrong to none.” \n",
    "            ― William Shakespeare, All's Well That Ends Well\n",
    "\n",
    "            “All the world's a stage,\n",
    "            And all the men and women merely players;\n",
    "            They have their exits and their entrances;\n",
    "            And one man in his time plays many parts,\n",
    "            His acts being seven ages.” \n",
    "            ― William Shakespeare, As You Like It\n",
    "\n",
    "            \"How old are you,\" asked Jem, \"four-and-a-half?\"\n",
    "\n",
    "            \"Goin' on seven.\"\n",
    "\n",
    "            \"Shoot no wonder, then,\" said Jem, jerking his thumb at me. \"Scout yonder's been readin' ever since she was born, \n",
    "            and she ain't even started to school yet. You look right puny for goin' on seven.\"\n",
    "\n",
    "            \"I'm little but I'm old,\" he said.\n",
    "            - To Kill a Mockingbird\n",
    "\n",
    "            Le dîner, Clémence, Anaïs, Raphaël, Voilà !\n",
    "\n",
    "            something... is! not right() with.,; this :: line.\n",
    "            \n",
    "            &nbsp;&nbsp;\n",
    "            \n",
    "            11    42   1024   2048\n",
    "            {{There are double curly braces.}}\n",
    "            {Here are single curly braces.}\n",
    "            </body>\n",
    "            </html>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mj6-7g_WwbbI"
   },
   "source": [
    "# Noise Removal\n",
    "\n",
    "Let's define noise removal as text-specific normalization tasks which often take place prior to tokenization. \n",
    "- While the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.\n",
    "\n",
    "Noise removal tasks could include:\n",
    "\n",
    "- Removing text file headers, footers\n",
    "- Removing HTML, XML, etc. markup and metadata\n",
    "- Extracting valuable data from other formats, such as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OybtmpuJxAbF"
   },
   "outputs": [],
   "source": [
    "# Write the code to remove all the html tags from the text string. And print the processed text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eE9JDVmMGR_t"
   },
   "source": [
    "While not mandatory to do at this stage prior to tokenization but:\n",
    "- Replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\"\n",
    "- It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIZoPnS-GmGS"
   },
   "outputs": [],
   "source": [
    "# Write the code to replace all the contractions. (I'm  ==>>  I am and so on.) [Hint: use contractions library.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xk7FdvKSHM-u"
   },
   "source": [
    "# Tokenization\n",
    " \n",
    "- Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. \n",
    "- Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. \n",
    "- Further processing is generally performed after a piece of text has been appropriately tokenized. \n",
    "- Tokenization is also referred to as text segmentation or lexical analysis.\n",
    "- Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words.\n",
    "\n",
    "### For our task, we will tokenize our sample text into a list of words. This is done using NTLK's word_tokenize() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6isuJOhmxHmx"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text and print.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6txvsPaw6FAT"
   },
   "source": [
    "# Normalization\n",
    "\n",
    "- converting all text to the same case (upper or lower), removing punctuation,  and so on.\n",
    "\n",
    "- Steps:\n",
    "  - Removal of non-ASCII characters.\n",
    "  - Conversion of all characters to lowercase.\n",
    "  - Removal of Punctuation.\n",
    "  - Stop word removal.\n",
    "  - Stemming / Lemmatization\n",
    "\n",
    "- After tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgLuyWs2IchN"
   },
   "source": [
    "Converting all words to lowercase and removing punctuations.\n",
    "\n",
    "**Stemming:** Converting the words into their base word or stem word ( Ex - tastefully, tasty, these words are converted to stem word called 'tasti'). This reduces the vector dimension because we dont consider all similar words\n",
    "\n",
    "**Stopwords:** Stopwords are the unnecessary words that even if they are removed the sentiment of the sentence dosent change.\n",
    "\n",
    "Ex - **This pasta is so tasty** ==> **pasta tasty** ( This , is, so are stopwords so they are removed)\n",
    "\n",
    "Hint:\n",
    "\n",
    "- Use regular expressions to remove punctuations.\n",
    "\n",
    "To see all the steps, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tbp1Y-OVyAi3"
   },
   "outputs": [],
   "source": [
    "# Write function to remove non-ASCII characters from the list of tokenized words.\n",
    "\n",
    "\n",
    "# Write function to convert all the characters from the list of tokenized words.\n",
    "\n",
    "\n",
    "# Write function to remove punctuations from the list of tokenized words.\n",
    "\n",
    "\n",
    "# Write function to remove stopwords from the list of tokenized words.\n",
    "\n",
    "\n",
    "# Write function to convert to stem words from the list of tokenized words.\n",
    "\n",
    "\n",
    "# Write function to lemmatize the words from the list of tokenized words.\n",
    "\n",
    "\n",
    "# write a function to perform all the above steps.\n",
    "\n",
    "\n",
    "# write the code to execute the function which has all the above steps combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vTaG2mA7FOf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practice Exercise: NLP Week-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
